{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Playground\n",
    "\n",
    "Interactive notebook: tokenization, embeddings, 2D visualization, and similarity search.\n",
    "\n",
    "Uses `sentence-transformers` for embeddings and `transformers` tokenizers for tokenization. UI is built with `ipywidgets`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install required packages (run once)\n",
    "!pip install -q sentence-transformers transformers tokenizers scikit-learn matplotlib plotly ipywidgets\n",
    "\n",
    "# If using JupyterLab you might need to install the ipywidgets labextension separately.\n",
    "print('Install finished (if output above shows success). Restart kernel if required.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import plotly.express as px\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "import math\n",
    "\n",
    "print('Imports OK')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load default embedding and tokenizer\n",
    "EMBED_MODEL_NAME = 'all-MiniLM-L6-v2'  # small, fast, good for demos\n",
    "embed_model = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2', use_fast=True)\n",
    "print('Loaded embedding model and tokenizer')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def tokenize_text(text, model_tokenizer=tokenizer):\n",
    "    \"\"\"Return tokens and token ids using the selected tokenizer.\"\"\"\n",
    "    enc = model_tokenizer(text, return_tensors=None)\n",
    "    # For GPT2 tokenizer object returns 'input_ids' list\n",
    "    ids = enc.get('input_ids')\n",
    "    if ids is None:\n",
    "        # fallback to encode single string\n",
    "        ids = model_tokenizer.encode(text)\n",
    "    # decode tokens individually (fast) -- for some tokenizers you can use tokenizer.convert_ids_to_tokens\n",
    "    try:\n",
    "        tokens = model_tokenizer.convert_ids_to_tokens(ids)\n",
    "    except Exception:\n",
    "        tokens = [str(i) for i in ids]\n",
    "    return tokens, ids\n",
    "\n",
    "def embed_texts(texts):\n",
    "    \"\"\"Return sentence-transformer embeddings (numpy array).\"\"\"\n",
    "    embs = embed_model.encode(texts, convert_to_numpy=True)\n",
    "    return embs\n",
    "\n",
    "def reduce_dim(embs, method='pca', n_components=2, random_state=42):\n",
    "    if embs.shape[0] <= 2:\n",
    "        # nothing to reduce\n",
    "        return embs[:, :n_components]\n",
    "    if method == 'pca':\n",
    "        p = PCA(n_components=n_components, random_state=random_state)\n",
    "        return p.fit_transform(embs)\n",
    "    else:\n",
    "        tsne = TSNE(n_components=n_components, random_state=random_state, init='pca')\n",
    "        return tsne.fit_transform(embs)\n",
    "\n",
    "def top_k_sim(query_emb, emb_matrix, k=5):\n",
    "    sims = cosine_similarity([query_emb], emb_matrix)[0]\n",
    "    idx = np.argsort(sims)[::-1][:k]\n",
    "    return idx, sims[idx]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample corpus\n",
    "We'll include a small default corpus you can extend."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sample_corpus = [\n",
    "    'I love machine learning and natural language processing.',\n",
    "    'Transformers are great for NLP tasks like translation and summarization.',\n",
    "    'The quick brown fox jumps over the lazy dog.',\n",
    "    'PyTorch and TensorFlow are popular deep learning frameworks.',\n",
    "    'Artificial intelligence will change how we work and learn.'\n",
    "]\n",
    "\n",
    "corpus_embs = embed_texts(sample_corpus)\n",
    "print('Sample corpus embedded, shape:', corpus_embs.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UI: Tokenize / Embed / Visualize / Similarity search"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Widgets\n",
    "text_in = widgets.Textarea(value='Enter text here', layout=widgets.Layout(width='80%', height='80px'))\n",
    "btn_tokenize = widgets.Button(description='Tokenize')\n",
    "btn_embed = widgets.Button(description='Embed & Visualize')\n",
    "method_dropdown = widgets.Dropdown(options=['pca', 'tsne'], value='pca', description='Reduce:')\n",
    "k_slider = widgets.IntSlider(value=3, min=1, max=10, description='Top k:')\n",
    "query_in = widgets.Text(value='', placeholder='Type an arbitrary query to search the corpus', description='Query:')\n",
    "btn_query = widgets.Button(description='Search Corpus')\n",
    "\n",
    "out = widgets.Output()\n",
    "\n",
    "def on_tokenize(b):\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        txt = text_in.value\n",
    "        tokens, ids = tokenize_text(txt)\n",
    "        display(Markdown(f\"**Text:** {txt}\"))\n",
    "        display(Markdown(f\"**Tokens ({len(tokens)}):** `{tokens}`\"))\n",
    "        display(Markdown(f\"**Token IDs:** `{ids}`\"))\n",
    "\n",
    "def on_embed(b):\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        txt = text_in.value\n",
    "        embs = embed_texts([txt] + sample_corpus)\n",
    "        reduced = reduce_dim(embs, method=method_dropdown.value)\n",
    "        df = {\n",
    "            'label': ['INPUT'] + [f'CORP_{i}' for i in range(len(sample_corpus))],\n",
    "            'text': [txt] + sample_corpus,\n",
    "            'x': reduced[:,0],\n",
    "            'y': reduced[:,1]\n",
    "        }\n",
    "        fig = px.scatter(df, x='x', y='y', hover_name='label', hover_data=['text'])\n",
    "        fig.update_layout(title='2D visualization of input + corpus (reduced)')\n",
    "        fig.show()\n",
    "\n",
    "def on_query(b):\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        q = query_in.value\n",
    "        if len(q.strip()) == 0:\n",
    "            print('Enter a query to search the corpus')\n",
    "            return\n",
    "        q_emb = embed_texts([q])[0]\n",
    "        idxs, sims = top_k_sim(q_emb, corpus_embs, k=k_slider.value)\n",
    "        for rank,(i,s) in enumerate(zip(idxs, sims), start=1):\n",
    "            display(Markdown(f\"**{rank}.** (score={s:.4f}) â€” {sample_corpus[i]}\"))\n",
    "\n",
    "btn_tokenize.on_click(on_tokenize)\n",
    "btn_embed.on_click(on_embed)\n",
    "btn_query.on_click(on_query)\n",
    "\n",
    "controls = widgets.HBox([btn_tokenize, btn_embed, method_dropdown])\n",
    "search_row = widgets.HBox([query_in, k_slider, btn_query])\n",
    "display(text_in, controls, search_row, out)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extend the corpus\n",
    "You can append to `sample_corpus` and re-run the embedding steps. Try pasting paragraphs, questions, or short documents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
